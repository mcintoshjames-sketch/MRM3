{
    "categories": [
        {
            "category_id": "model_calibration",
            "category_name": "Model calibration",
            "kpis": [
                {
                    "kpi_name": "Brier Score",
                    "kpi_description": "Mean squared error between predicted probabilities and actual binary outcomes; reflects both calibration and overall probabilistic accuracy.",
                    "kpi_calculation": "Average over all observations of (predicted_probability - actual_outcome)^2, where actual_outcome is 0 or 1.",
                    "kpi_interpretation": "Lower is better; 0 indicates perfect calibration. As a rule of thumb, a model that is materially better than a naive constant-probability model (event_rate * (1 - event_rate)) is acceptable. Rising Brier score over time suggests worsening calibration.",
                    "kpi_group_id": "model_calibration",
                    "kpi_group_name": "Model calibration"
                },
                {
                    "kpi_name": "Hosmer-Lemeshow Test",
                    "kpi_description": "Goodness-of-fit test for binary probability models; checks whether observed event rates agree with predicted probabilities across risk bands.",
                    "kpi_calculation": "Sort observations by predicted probability, split into K groups (often 10), then compute a chi-square statistic comparing observed versus expected events in each group to obtain a p-value.",
                    "kpi_interpretation": "High p-value (for example > 0.05) indicates no evidence of miscalibration; very low p-value (for example < 0.01) suggests systematic mismatch between predicted and observed event rates and may trigger recalibration.",
                    "kpi_group_id": "model_calibration",
                    "kpi_group_name": "Model calibration"
                },
                {
                    "kpi_name": "Calibration Intercept",
                    "kpi_description": "Measures overall bias of predicted probabilities relative to actual outcomes (calibration in the large).",
                    "kpi_calculation": "Fit a logistic regression of outcomes on the log-odds of predicted probabilities with slope fixed at 1; the constant term is the calibration intercept.",
                    "kpi_interpretation": "Ideal value is 0. A positive intercept means the model underestimates risk on average (events happen more often than predicted); a negative intercept means the model overestimates risk. In practice, values within about +/-0.1 are usually considered acceptable, with larger deviations suggesting recalibration.",
                    "kpi_group_id": "model_calibration",
                    "kpi_group_name": "Model calibration"
                },
                {
                    "kpi_name": "Calibration Slope",
                    "kpi_description": "Assesses whether predicted probabilities are too extreme or too moderate compared to actual outcomes.",
                    "kpi_calculation": "Fit a logistic regression of outcomes on the log-odds of predicted probabilities (with intercept); the coefficient on the log-odds term is the calibration slope.",
                    "kpi_interpretation": "Ideal value is 1. A slope below 1 indicates over-confident predictions (high risks too high, low risks too low); a slope above 1 indicates under-confident predictions. Slopes between about 0.8 and 1.2 are commonly viewed as reasonable; values much outside this range indicate substantial miscalibration.",
                    "kpi_group_id": "model_calibration",
                    "kpi_group_name": "Model calibration"
                },
                {
                    "kpi_name": "Expected Calibration Error (ECE)",
                    "kpi_description": "Summarises, in a single value, the average absolute difference between predicted probabilities and observed event rates across bins of prediction confidence.",
                    "kpi_calculation": "Bin predictions into intervals (for example deciles of predicted probability). For each bin, compute the absolute difference between the mean predicted probability and the observed event rate. ECE is the weighted average of these differences, weighted by bin size.",
                    "kpi_interpretation": "Ranges from 0 upward; lower is better. Values of a few percentage points (for example ECE < 0.02) usually indicate very good calibration. Increasing ECE over time is a sign of calibration drift and may trigger recalibration or model refresh.",
                    "kpi_group_id": "model_calibration",
                    "kpi_group_name": "Model calibration"
                }
            ]
        },
        {
            "category_id": "model_performance",
            "category_name": "Model performance",
            "kpis": [
                {
                    "kpi_name": "Accuracy",
                    "kpi_description": "Overall proportion of correctly classified observations.",
                    "kpi_calculation": "Accuracy = (true_positives + true_negatives) / (true_positives + false_positives + true_negatives + false_negatives).",
                    "kpi_interpretation": "Ranges from 0 to 1; higher is better. Useful when classes are roughly balanced. In imbalanced problems (for example fraud detection), accuracy can be misleading and should be interpreted together with precision, recall and other metrics.",
                    "kpi_group_id": "model_performance",
                    "kpi_group_name": "Model performance"
                },
                {
                    "kpi_name": "Precision",
                    "kpi_description": "Share of predicted positives that are actually positive; measures how often alerts are correct.",
                    "kpi_calculation": "Precision = true_positives / (true_positives + false_positives).",
                    "kpi_interpretation": "Ranges from 0 to 1; higher is better. High precision means few false alarms (for example, most flagged fraud cases are truly fraud). Operationally, institutions may target precision above 0.8 or 0.9 for certain high-cost review processes.",
                    "kpi_group_id": "model_performance",
                    "kpi_group_name": "Model performance"
                },
                {
                    "kpi_name": "Recall",
                    "kpi_description": "Share of actual positives that the model correctly identifies; also called sensitivity or true positive rate.",
                    "kpi_calculation": "Recall = true_positives / (true_positives + false_negatives).",
                    "kpi_interpretation": "Ranges from 0 to 1; higher is better. High recall means the model misses few bad cases (for example few defaulters slip through). For risk models, recall above 0.9 on key segments is often desirable, subject to cost trade-offs.",
                    "kpi_group_id": "model_performance",
                    "kpi_group_name": "Model performance"
                },
                {
                    "kpi_name": "F1 Score",
                    "kpi_description": "Harmonic mean of precision and recall; balances the trade-off between missing positives and generating false alarms.",
                    "kpi_calculation": "F1 = 2 * precision * recall / (precision + recall).",
                    "kpi_interpretation": "Ranges from 0 to 1; higher is better. An F1 score near 1 indicates both high precision and high recall. In imbalanced settings, F1 is often used as a primary optimisation metric; values above about 0.7 are usually regarded as good, depending on the use case.",
                    "kpi_group_id": "model_performance",
                    "kpi_group_name": "Model performance"
                },
                {
                    "kpi_name": "ROC AUC",
                    "kpi_description": "Area under the receiver operating characteristic curve; measures the model’s ability to rank positive cases above negative cases across all thresholds.",
                    "kpi_calculation": "Compute true positive rate and false positive rate for many score thresholds, plot the ROC curve and calculate the area under the curve.",
                    "kpi_interpretation": "Ranges from 0.5 (no discrimination) to 1.0 (perfect discrimination). In credit risk, ROC AUC around 0.7 to 0.8 is common; above 0.8 is strong. Declines in ROC AUC over time indicate reduced discriminatory power.",
                    "kpi_group_id": "model_performance",
                    "kpi_group_name": "Model performance"
                },
                {
                    "kpi_name": "PR AUC",
                    "kpi_description": "Area under the precision-recall curve; focuses on performance on the positive (often rare) class.",
                    "kpi_calculation": "Compute precision and recall for many thresholds, plot the curve and calculate the area under it.",
                    "kpi_interpretation": "Ranges from 0 to 1; higher is better. The baseline PR AUC equals the prevalence of the positive class. For very rare events (for example 1% fraud), a PR AUC far above 0.01 (for example 0.2 or greater) indicates a highly effective model.",
                    "kpi_group_id": "model_performance",
                    "kpi_group_name": "Model performance"
                },
                {
                    "kpi_name": "Kolmogorov-Smirnov (KS)",
                    "kpi_description": "Maximum separation between the cumulative score distributions of positive and negative classes; widely used in credit scoring.",
                    "kpi_calculation": "Sort records by predicted score; at each score threshold compute cumulative shares of positives and negatives. KS is the maximum difference between these two cumulative curves.",
                    "kpi_interpretation": "Ranges from 0 to 1 (often reported as 0 to 100%). Higher is better. In many credit models, KS above 0.3 (30%) is acceptable, around 0.4 to 0.5 is considered strong. A declining KS over time indicates weakening rank-order stability.",
                    "kpi_group_id": "model_performance",
                    "kpi_group_name": "Model performance"
                },
                {
                    "kpi_name": "Log Loss (Cross-Entropy)",
                    "kpi_description": "Penalty-based measure of how well predicted probabilities match actual outcomes; strongly penalises confident wrong predictions.",
                    "kpi_calculation": "For binary classification, log_loss = -(1/N) * sum( y * log(p) + (1 - y) * log(1 - p) ) over all observations, where p is predicted probability and y is 0 or 1.",
                    "kpi_interpretation": "Lower is better; 0 is perfect. A log loss close to the entropy of the class distribution corresponds to a weak or uninformative model. Rising log loss over time, with similar class mix, suggests deterioration in either calibration or discrimination.",
                    "kpi_group_id": "model_performance",
                    "kpi_group_name": "Model performance"
                },
                {
                    "kpi_name": "Mean Squared Error (MSE)",
                    "kpi_description": "Standard regression metric measuring average squared difference between predicted values and actual values.",
                    "kpi_calculation": "MSE = (1/N) * sum( (prediction - actual)^2 ) across all observations; root mean squared error (RMSE) is the square root of MSE.",
                    "kpi_interpretation": "Lower is better; 0 indicates perfect prediction. Because MSE is in squared units, RMSE is often used for interpretability. Acceptable values depend on the scale of the target; for example, for monetary predictions, RMSE should be small relative to typical transaction sizes.",
                    "kpi_group_id": "model_performance",
                    "kpi_group_name": "Model performance"
                }
            ]
        },
        {
            "category_id": "model_input_data_monitoring",
            "category_name": "Model input data monitoring",
            "kpis": [
                {
                    "kpi_name": "Missing Data Rate",
                    "kpi_description": "Proportion of missing or null values in model input features.",
                    "kpi_calculation": "For each feature, missing_rate = number_of_missing_values / total_number_of_records; can also aggregate across features.",
                    "kpi_interpretation": "Ideally low and stable over time. Sudden increases often indicate upstream data issues. Many institutions flag if any key feature’s missing rate exceeds a threshold (for example 5%) compared with training.",
                    "kpi_group_id": "model_input_data_monitoring",
                    "kpi_group_name": "Model input data monitoring"
                },
                {
                    "kpi_name": "New Category Rate",
                    "kpi_description": "Frequency with which categorical features take values that were not seen during model training.",
                    "kpi_calculation": "For each categorical variable, new_category_rate = records_with_unseen_value / total_records in the monitoring window.",
                    "kpi_interpretation": "Should be close to 0; any persistent non-zero values for important features indicate population change or data quality problems and may require model updates or revised encodings.",
                    "kpi_group_id": "model_input_data_monitoring",
                    "kpi_group_name": "Model input data monitoring"
                },
                {
                    "kpi_name": "Feature Distribution Drift (PSI)",
                    "kpi_description": "Population Stability Index computed on individual input features to quantify distribution shift between a baseline period and current data.",
                    "kpi_calculation": "Bin each feature using bins defined on baseline data. For each bin, compute baseline and current proportions, then sum over bins: (current - baseline) * ln(current / baseline).",
                    "kpi_interpretation": "Common rules of thumb: PSI < 0.1 indicates little change; 0.1 to 0.2 suggests moderate drift; > 0.2 indicates significant drift that may affect model performance. High PSI on key features is a trigger for investigation.",
                    "kpi_group_id": "model_input_data_monitoring",
                    "kpi_group_name": "Model input data monitoring"
                }
            ]
        },
        {
            "category_id": "model_stability",
            "category_name": "Model stability",
            "kpis": [
                {
                    "kpi_name": "Score Distribution PSI",
                    "kpi_description": "Population Stability Index computed on the model score or predicted probability to track shifts in the overall risk mix.",
                    "kpi_calculation": "Bin the model scores using bins defined on a reference period (for example training or prior year), then compute PSI between reference and current score distributions.",
                    "kpi_interpretation": "The same rule of thumb as for feature PSI applies: < 0.1 stable, 0.1 to 0.2 moderate shift, > 0.2 large shift. Persistent high score PSI often indicates a change in portfolio composition or macroeconomic conditions, and may precede performance degradation.",
                    "kpi_group_id": "model_stability",
                    "kpi_group_name": "Model stability"
                },
                {
                    "kpi_name": "Performance Drift (Metric Change)",
                    "kpi_description": "Change in a chosen performance metric (for example ROC AUC, KS, F1) between a baseline period and the current monitoring period.",
                    "kpi_calculation": "Compute the performance metric on recent data and subtract the baseline value, or express the difference as a percentage of the baseline.",
                    "kpi_interpretation": "Negative changes indicate deterioration. Tolerances depend on the metric and use case; for example, an AUC drop of more than 0.03 or a relative drop in KS greater than about 10% may warrant investigation or model recalibration.",
                    "kpi_group_id": "model_stability",
                    "kpi_group_name": "Model stability"
                }
            ]
        },
        {
            "category_id": "global_interpretability",
            "category_name": "Global interpretability",
            "kpis": [
                {
                    "kpi_name": "Feature Importance",
                    "kpi_description": "Relative contribution of each feature to the model’s predictions when considered over the entire portfolio.",
                    "kpi_calculation": "Model-specific or model-agnostic methods, such as tree-based impurity reduction, permutation importance, or mean absolute SHAP values aggregated across all observations.",
                    "kpi_interpretation": "Features with larger importance scores are the main drivers of model behaviour. Important features should be conceptually sensible and stable over time; sudden shifts in importance profiles may indicate changes in data or model specification that need explanation.",
                    "kpi_group_id": "global_interpretability",
                    "kpi_group_name": "Global interpretability"
                },
                {
                    "kpi_name": "Surrogate Model Fidelity",
                    "kpi_description": "How closely a simpler, interpretable surrogate model can reproduce the predictions of the complex model.",
                    "kpi_calculation": "Train a simple model (for example shallow tree, linear model, or scorecard) to predict the complex model’s outputs instead of the true labels; compute R-squared, accuracy, or another similarity metric between surrogate predictions and complex model predictions.",
                    "kpi_interpretation": "Higher fidelity (for example R-squared or agreement above 0.8) indicates that the complex model’s logic can be well-approximated and explained globally. Low fidelity suggests that simple global explanations may be misleading, and that local or more complex explanation techniques are required.",
                    "kpi_group_id": "global_interpretability",
                    "kpi_group_name": "Global interpretability"
                },
                {
                    "kpi_name": "Model Complexity",
                    "kpi_description": "Structural complexity of the model, used as a proxy for ease of understanding and ability to validate.",
                    "kpi_calculation": "Depends on model type: number of features with non-zero coefficients, number and depth of trees, number of rules, parameters, or layers, etc.",
                    "kpi_interpretation": "There is no universal numeric threshold, but lower complexity generally aids interpretability and governance. Many institutions set explicit limits (for example maximum number of features or tree depth) and flag models that exceed those limits for additional review.",
                    "kpi_group_id": "global_interpretability",
                    "kpi_group_name": "Global interpretability"
                }
            ]
        },
        {
            "category_id": "local_interpretability",
            "category_name": "Local interpretability",
            "kpis": [
                {
                    "kpi_name": "Local Explanation Fidelity",
                    "kpi_description": "How accurately a local explanation (for example LIME, local SHAP) reproduces the complex model’s predictions in a small neighbourhood around an individual case.",
                    "kpi_calculation": "For a given observation, generate perturbed samples around it, fit a simple local surrogate model, and compute R-squared or classification accuracy between surrogate predictions and the complex model’s predictions on those samples.",
                    "kpi_interpretation": "Higher values indicate more trustworthy explanations. As a guideline, local fidelity above about 0.8 is generally considered good; low-fidelity explanations should not be used for sensitive decisions without further analysis.",
                    "kpi_group_id": "local_interpretability",
                    "kpi_group_name": "Local interpretability"
                },
                {
                    "kpi_name": "Local Explanation Stability",
                    "kpi_description": "Degree to which explanations for similar inputs remain consistent.",
                    "kpi_calculation": "Compare explanations (for example top contributing features or attribution vectors) for the same case under small perturbations or repeated runs, or for very similar cases; quantify similarity using overlap or correlation measures.",
                    "kpi_interpretation": "High stability means small changes in input or random seeds do not materially alter the explanation, supporting trust. Large instability suggests that explanations may be fragile and should be treated with caution in customer or regulator-facing contexts.",
                    "kpi_group_id": "local_interpretability",
                    "kpi_group_name": "Local interpretability"
                },
                {
                    "kpi_name": "Explanation Sparsity",
                    "kpi_description": "Number of features required to explain an individual prediction to a reasonable degree.",
                    "kpi_calculation": "Count the number of features whose local importance or attribution exceeds a chosen threshold, or fix the explanation length (for example top 3 to 5 features) and measure how much of the prediction is captured.",
                    "kpi_interpretation": "Lower sparsity (fewer features) typically improves interpretability. Many financial institutions prefer explanations that rely on a small set of intuitive drivers (for example 3 to 7 variables) for any one decision.",
                    "kpi_group_id": "local_interpretability",
                    "kpi_group_name": "Local interpretability"
                }
            ]
        },
        {
            "category_id": "llm_monitoring",
            "category_name": "LLM monitoring",
            "kpis": [
                {
                    "kpi_name": "Perplexity",
                    "kpi_description": "Measures how well a language model predicts text; lower perplexity means better next-token prediction on a reference corpus.",
                    "kpi_calculation": "Perplexity is the exponential of the average negative log-likelihood of the correct token over a set of sequences.",
                    "kpi_interpretation": "Lower is better, but absolute values depend on the task and dataset. For monitoring, the main signal is change over time: rising perplexity on a fixed evaluation set suggests degradation in language modelling quality.",
                    "kpi_group_id": "llm_monitoring",
                    "kpi_group_name": "LLM monitoring"
                },
                {
                    "kpi_name": "Factual Accuracy",
                    "kpi_description": "Proportion of model responses that are factually correct and sufficiently supported by trusted sources.",
                    "kpi_calculation": "On a set of prompts with known answers or verifiable ground truth, measure the fraction of responses assessed as fully correct; may be evaluated by human review or automated fact-checking.",
                    "kpi_interpretation": "Higher is better; in financial customer-facing use cases, targets may be above 95% or higher, with zero tolerance for errors on critical regulatory or product information.",
                    "kpi_group_id": "llm_monitoring",
                    "kpi_group_name": "LLM monitoring"
                },
                {
                    "kpi_name": "Toxicity Score",
                    "kpi_description": "Extent to which model outputs contain abusive, hateful, or otherwise inappropriate language.",
                    "kpi_calculation": "Run outputs through a toxicity classifier or rule-based detector and record either the average toxicity score or the proportion of outputs whose score exceeds a defined threshold.",
                    "kpi_interpretation": "Should be extremely low in production. Many institutions require that virtually no outputs exceed the toxicity threshold; any spike triggers investigation into prompts, guardrails, or model configuration.",
                    "kpi_group_id": "llm_monitoring",
                    "kpi_group_name": "LLM monitoring"
                },
                {
                    "kpi_name": "Response Latency",
                    "kpi_description": "End-to-end time taken for the model to produce a response to a user query.",
                    "kpi_calculation": "Measure wall-clock time between receipt of input and completion of output, and summarise using averages and high percentiles (for example p90, p95).",
                    "kpi_interpretation": "Lower is better for user experience. Typical targets for interactive systems are in the range of sub-second to a few seconds; sustained breaches of service-level targets indicate the need for optimisation or scaling.",
                    "kpi_group_id": "llm_monitoring",
                    "kpi_group_name": "LLM monitoring"
                }
            ]
        },
        {
            "category_id": "fairness_and_governance",
            "category_name": "Fairness and governance",
            "kpis": [
                {
                    "kpi_name": "Disparate Impact Ratio",
                    "kpi_description": "Ratio of positive outcome rates between a protected group and a reference group; used to assess potential adverse impact.",
                    "kpi_calculation": "Compute the proportion of approved or positive decisions in the protected group and in the reference group, then take protected_rate / reference_rate.",
                    "kpi_interpretation": "Values close to 1 indicate parity. In many jurisdictions, a ratio below about 0.8 (the 80 percent rule) is treated as a potential red flag for adverse impact and requires further analysis and justification.",
                    "kpi_group_id": "fairness_and_governance",
                    "kpi_group_name": "Fairness and governance"
                },
                {
                    "kpi_name": "Equal Opportunity Difference",
                    "kpi_description": "Difference in true positive rates between a protected group and a reference group for a favourable outcome.",
                    "kpi_calculation": "Compute recall (true positive rate) separately for the protected and reference groups, then subtract reference_group_recall from protected_group_recall.",
                    "kpi_interpretation": "The ideal value is 0, meaning equal ability to correctly identify positives across groups. Differences of only a few percentage points may be acceptable; larger gaps can indicate fairness concerns that need mitigation or policy review.",
                    "kpi_group_id": "fairness_and_governance",
                    "kpi_group_name": "Fairness and governance"
                }
            ]
        }
    ]
}